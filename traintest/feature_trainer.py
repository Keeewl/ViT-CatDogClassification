import sys
import os

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„ï¼Œå†å‘ä¸Šè·³è½¬ä¸€çº§ï¼ˆå›åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼‰
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

import numpy as np
import time
import matplotlib.pyplot as plt

from model.vit_config import VITConfig
from model.mlp_head import MLPHead


config = VITConfig()


# === FeatureTrainerï¼ˆä½¿ç”¨æå–çš„CLSç‰¹å¾è®­ç»ƒMLP Headï¼‰ ===
class FeatureTrainer:
    def __init__(self, head, X_train, y_train, X_test, y_test, config):
        """
        åªè®­ç»ƒMLP Headï¼Œä¸ä¾èµ–ViTä¸»å¹²æ¨¡å—
        :param head: MLPHead å®ä¾‹
        :param X_train: æå–åçš„è®­ç»ƒç‰¹å¾ [N, 768]
        :param y_train: è®­ç»ƒæ ‡ç­¾ [N]
        :param X_test: æå–åçš„æµ‹è¯•ç‰¹å¾ [N, 768]
        :param y_test: æµ‹è¯•æ ‡ç­¾ [N]
        :param config: é…ç½®å¯¹è±¡ï¼ˆå­¦ä¹ ç‡ã€batch size ç­‰ï¼‰
        """
        self.head = head
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
        self.lr = config.learning_rate
        self.batch_size = config.batch_size
        self.num_epochs = config.num_epochs

        self.weight_decay = config.weight_decay

        # ä¿å­˜æƒé‡å¼•ç”¨
        self.params = {
            'W1': head.W1,
            'b1': head.b1,
            'W2': head.W2,
            'b2': head.b2
        }

        # åˆå§‹åŒ–æ¢¯åº¦ç¼“å­˜
        self.grads = {
            'W1': np.zeros_like(head.W1),
            'b1': np.zeros_like(head.b1),
            'W2': np.zeros_like(head.W2),
            'b2': np.zeros_like(head.b2)
        }

        # è®°å½•
        self.train_loss_history = []
        self.train_acc_history = []
        self.test_acc_history = []

    def _forward_pass(self, X_batch, y_batch):
        """
        å‰å‘ä¼ æ’­ï¼šMLP Head è®¡ç®— logitsã€lossã€acc
        """
        logits = self.head.forward(X_batch, training=True)

        # Softmax
        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))
        probs = exps / np.sum(exps, axis=1, keepdims=True)

        # Cross entropy loss
        B = y_batch.shape[0]
        correct_logprobs = -np.log(probs[np.arange(B), y_batch])
        loss = np.mean(correct_logprobs)

        preds = np.argmax(probs, axis=1)
        acc = np.mean(preds == y_batch)

        return logits, loss, acc, probs

    def _backward_pass(self, X_batch, probs, y_batch):
        """
        åå‘ä¼ æ’­æ›´æ–° MLP Head çš„æ¢¯åº¦
        :param X_batch: CLSç‰¹å¾ [B, 768]
        :param probs: Softmax æ¦‚ç‡ [B, 2]
        """
        B = y_batch.shape[0]
        dlogits = probs.copy()
        dlogits[np.arange(B), y_batch] -= 1
        dlogits /= B

        # W2, b2
        h1 = self.head.hidden
        self.grads['W2'] = h1.T @ dlogits
        self.grads['b2'] = np.sum(dlogits, axis=0, keepdims=True)

        # # GELU åä¼ 
        # dh1 = dlogits @ self.head.W2.T
        # x1 = self.head.hidden_input
        # gelu_grad = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x1 + 0.044715 * x1**3)))
        # dh1 *= gelu_grad
        #
        # if self.head.dropout_rate > 0:
        #     dh1 *= self.head.dropout_mask / (1.0 - self.head.dropout_rate)

        # GELU åä¼ ï¼ˆtanh è¿‘ä¼¼çš„å®Œæ•´å¯¼æ•°ï¼‰
        dh1 = dlogits @ self.head.W2.T
        x1 = self.head.hidden_input

        c = np.float32(np.sqrt(2.0 / np.pi))
        t = c * (x1 + 0.044715 * (x1 ** 3))
        tanh_t = np.tanh(t)
        sech2_t = 1.0 - tanh_t ** 2  # = sech(t)^2
        dt_dx = c * (1.0 + 3.0 * 0.044715 * (x1 ** 2))

        gelu_grad = 0.5 * (1.0 + tanh_t) + 0.5 * x1 * sech2_t * dt_dx
        dh1 *= gelu_grad

        # Dropout åä¼ ï¼ˆä¸å‰å‘çš„ç¼©æ”¾ä¸€è‡´ï¼‰
        if self.head.dropout_rate > 0:
            dh1 *= self.head.dropout_mask / (1.0 - self.head.dropout_rate)

        # W1, b1
        self.grads['W1'] = X_batch.T @ dh1
        self.grads['b1'] = np.sum(dh1, axis=0, keepdims=True)

    def _update_parameters(self):
        # """
        # SGD æ›´æ–°å‚æ•°
        # """
        # for name in self.params:
        #     self.params[name] -= self.lr * self.grads[name]

        wd = self.weight_decay # e.g. 1e-4
        # å¯¹æƒé‡åš L2ï¼ˆä¸å¯¹åç½®ï¼‰
        for name in ('W1', 'W2'):
            self.params[name] -= self.lr * (self.grads[name] + wd * self.params[name])
        for name in ('b1', 'b2'):
            self.params[name] -= self.lr * self.grads[name]

    def train(self):
        """
        ä¸»è®­ç»ƒå¾ªç¯
        """
        num_samples = self.X_train.shape[0]
        num_batches = int(np.ceil(num_samples / self.batch_size))

        print(f"å¼€å§‹è®­ç»ƒ MLP Headï¼Œå…± {self.num_epochs} ä¸ª epochï¼Œæ¯ä¸ª epoch æœ‰ {num_batches} ä¸ª batch")

        for epoch in range(1, self.num_epochs + 1):
            indices = np.arange(num_samples)
            np.random.shuffle(indices)
            X_train_shuffled = self.X_train[indices]
            y_train_shuffled = self.y_train[indices]

            epoch_loss, epoch_acc = 0.0, 0.0

            for i in range(num_batches):
                start = i * self.batch_size
                end = min(start + self.batch_size, num_samples)
                X_batch = X_train_shuffled[start:end]
                y_batch = y_train_shuffled[start:end]

                logits, loss, acc, probs = self._forward_pass(X_batch, y_batch)
                self._backward_pass(X_batch, probs, y_batch)
                self._update_parameters()

                epoch_loss += loss
                epoch_acc += acc

            avg_loss = epoch_loss / num_batches
            avg_acc = epoch_acc / num_batches
            self.train_loss_history.append(avg_loss)
            self.train_acc_history.append(avg_acc)

            test_acc = self.evaluate()
            self.test_acc_history.append(test_acc)

            print(f"[Epoch {epoch}] Train Loss: {avg_loss:.4f}, Train Acc: {avg_acc:.4f}, Test Acc: {test_acc:.4f}")

    def evaluate(self):
        logits = self.head.forward(self.X_test, training=False)  # â† å…³é—­dropout
        preds = np.argmax(logits, axis=1)
        acc = np.mean(preds == self.y_test)
        return acc

    def plot_history(self):
        """
        è®­ç»ƒå¯è§†åŒ–
        """
        epochs = range(1, self.num_epochs + 1)
        plt.figure(figsize=(14, 5))

        plt.subplot(1, 2, 1)
        plt.plot(epochs, self.train_loss_history, label='Train Loss', color='red')
        plt.plot(epochs, self.train_acc_history, label='Train Acc', color='blue')
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Training Loss Curve")
        plt.grid(True)
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(epochs, self.train_acc_history, label='Train Acc', color='blue')
        plt.plot(epochs, self.test_acc_history, label='Test Acc', color='green')
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.title("Accuracy Curve")
        plt.grid(True)
        plt.legend()

        plt.tight_layout()
        plt.show()

    def save_mlp_weights(self, save_path):
        """ä¿å­˜è®­ç»ƒåçš„MLP Headæƒé‡åˆ°æŒ‡å®šè·¯å¾„"""
        save_dir = os.path.dirname(save_path)
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
        np.savez(save_path,
                 W1=self.head.W1,
                 b1=self.head.b1,
                 W2=self.head.W2,
                 b2=self.head.b2)
        print(f"ğŸ’¾ MLP Headæƒé‡å·²ä¿å­˜è‡³: {save_path}")


# # FeatureTrainerç‰¹å¾è®­ç»ƒ
# if __name__ == "__main__":
#     print("===== FeatureTrainerç‰¹å¾è®­ç»ƒ =====")

#     # é€‰ä¸€ç§ç‰¹å¾ï¼š'gap' æˆ– 'concat'ï¼ˆä¹Ÿå¯å…ˆè·‘ gapï¼Œçœ‹æ›²çº¿åå†æ¢ concatï¼‰
#     FEAT_KIND = "cls"  # "gap" æˆ– "concat" æˆ– "cls"
#     # FEAT_KIND = "gap"  # "gap" æˆ– "concat" æˆ– "cls"

#     use_zscore = True  # å»ºè®® True

#     base = "features"
#     Xtr = np.load(f"{base}/{FEAT_KIND}_train_features{'_z' if use_zscore else ''}.npy")
#     Xte = np.load(f"{base}/{FEAT_KIND}_test_features{'_z' if use_zscore else ''}.npy")
#     ytr = np.load(f"{base}/y_train.npy")
#     yte = np.load(f"{base}/y_test.npy")

#     print(f"è®­ç»ƒç‰¹å¾å½¢çŠ¶: {Xtr.shape} | æµ‹è¯•ç‰¹å¾å½¢çŠ¶: {Xte.shape} | ç§ç±»: {FEAT_KIND}{'_z' if use_zscore else ''}")

#     # é…ç½®
#     # config = ViTConfig()
#     head = MLPHead(config)

#     start = time.time()
#     trainer = FeatureTrainer(head, Xtr, ytr, Xte, yte, config)
#     trainer.train()
#     end = time.time()
#     print("è€—æ—¶: {:.4f} ç§’".format(end - start))

#     trainer.plot_history()


# FeatureTrainerç‰¹å¾è®­ç»ƒ
if __name__ == "__main__":
    print("===== FeatureTrainerç‰¹å¾è®­ç»ƒ =====")

    base = "feature_test_50samples"
    Xtr = np.load(f"{base}/my_train_features.npy")
    Xte = np.load(f"{base}/my_test_features.npy")
    ytr = np.load(f"{base}/my_train_labels.npy")
    yte = np.load(f"{base}/my_test_labels.npy")

    print(f"è®­ç»ƒç‰¹å¾å½¢çŠ¶: {Xtr.shape} | æµ‹è¯•ç‰¹å¾å½¢çŠ¶: {Xte.shape}")

    head = MLPHead(config)

    start = time.time()
    trainer = FeatureTrainer(head, Xtr, ytr, Xte, yte, config)
    trainer.train()
    end = time.time()
    print("è€—æ—¶: {:.4f} ç§’".format(end - start))

    weights_path = os.path.join(project_root, "extract_weights", "mlp_head_trained_weights.npz")
    trainer.save_mlp_weights(weights_path)

    trainer.plot_history()
