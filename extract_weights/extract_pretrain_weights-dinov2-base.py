import os
import math
import argparse
import numpy as np
import torch
import torch.nn.functional as F
from transformers import Dinov2Model

"""
Dinov2-Base é¢„è®­ç»ƒæƒé‡å¯¼å‡ºï¼ˆç»Ÿä¸€ & æ›´æ–°ç‰ˆï¼‰
- ä¸€æ¬¡æ€§å¯¼å‡ºï¼šPatch Embeddingã€CLS tokenã€Position Embedding(æ’å€¼åˆ°16x16)ã€
  æ¯å±‚Encoderçš„ Q/K/Vã€O(out proj)ã€FC1/FC2ã€LayerNorm1/2ã€Encoderå°¾éƒ¨LayerNormï¼Œè¿˜æœ‰æ®‹å·®ç¼©æ”¾LayerScale
- ä¿æŒä¸ä½  NumPy æ‰‹æ“ViTï¼ˆå³ä¹˜ï¼‰ä¸€è‡´çš„æƒé‡æ–¹å‘ï¼šæ‰€æœ‰çº¿æ€§å±‚æƒé‡å‡å·²è½¬ç½®åä¿å­˜
- è°ƒç”¨æ–¹å¼ï¼ˆé»˜è®¤ä»æœ¬åœ° ./dinov2-base åŠ è½½ï¼Œä¿å­˜åˆ° ./weights_vit_base_224ï¼‰ï¼š
    python PretrainWeights-dinov2-base.py
  æˆ–è€…ï¼ˆä»HuggingFaceåœ¨çº¿åŠ è½½ï¼‰ï¼š
    python PretrainWeights-dinov2-base.py --model-id facebook/dinov2-bas
"""

# === å·¥å…·å‡½æ•° ===
"""
Python çš„ç±»å‹æ³¨è§£ï¼ˆtype hintsï¼‰è¯­æ³•ï¼š
t: torch.Tensorï¼šç»™å‚æ•° t åšç±»å‹æ ‡æ³¨ï¼Œè¡¨ç¤ºè¿™ä¸ªå‚æ•°æœŸæœ›æ˜¯ torch.Tensorã€‚
-> np.ndarrayï¼šç»™è¿”å›å€¼åšç±»å‹æ ‡æ³¨ï¼Œè¡¨ç¤ºå‡½æ•°åº”å½“è¿”å› numpy.ndarrayã€‚
"""
def _to_numpy(t: torch.Tensor) -> np.ndarray:
    return t.detach().cpu().numpy().astype(np.float32) # detach()ï¼šåˆ‡æ–­è®¡ç®—å›¾ï¼Œé˜²æ­¢ Autograd è·Ÿè¸ªåˆ°è¿™ä»½æ•°æ®ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰


def _save_np(path, arr, msg=None):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    np.save(path, arr)
    if msg is not None:
        print(msg, "shape =", tuple(arr.shape))


def _get_attr(obj, names): # objæ˜¯layerå±‚å¯¹è±¡
    """é²æ£’è·å–å±æ€§ï¼šæŒ‰ names é¡ºåºå°è¯•ï¼Œç›´åˆ°æœ‰ä¸€ä¸ªå­˜åœ¨"""
    for n in names:
        if hasattr(obj, n):
            return getattr(obj, n)
    return None


def interpolate_pos_embed_to_grid(pos_tokens_patch_np: np.ndarray, target_h: int, target_w: int) -> np.ndarray:
    """
    å°† [1, N, C] çš„ patch éƒ¨åˆ†ä½ç½®ç¼–ç é‡æ’åˆ° [1, C, H, W]ï¼Œç”¨åŒä¸‰æ¬¡æ’å€¼åˆ° (target_h, target_w)ï¼Œ
    å†è¿˜åŸä¸º [target_h, target_w, C] çš„äºŒç»´ç½‘æ ¼ï¼ˆä¾¿äº NumPy ç‰ˆç›´æ¥ä½¿ç”¨ï¼‰ã€‚
    """

    assert pos_tokens_patch_np.ndim == 3 and pos_tokens_patch_np.shape[0] == 1
    B, N, C = pos_tokens_patch_np.shape  # [1, 1369, 768] for 37x37
    src_hw = int(round(math.sqrt(N)))
    assert src_hw * src_hw == N, f"ä½ç½®ç¼–ç patchæ•°ä¸æ˜¯å®Œç¾å¹³æ–¹ï¼š{N}"

    # [1, N, C] -> [1, H, W, C] -> [1, C, H, W]
    pos_hw = pos_tokens_patch_np.reshape(1, src_hw, src_hw, C)
    pos_chw = np.transpose(pos_hw, (0, 3, 1, 2))  # [1, C, H, W]
    pos_chw_t = torch.from_numpy(pos_chw)

    # åŒä¸‰æ¬¡æ’å€¼
    with torch.no_grad():
        pos_resized = F.interpolate(pos_chw_t, size=(target_h, target_w), mode="bicubic", align_corners=False) # æŒ‰ç©ºé—´ç»´ (H, W) åŒä¸‰æ¬¡æ’å€¼åˆ° (target_h, target_w)ã€‚
        # ç¨ä½œè£å‰ªä»¥é˜²æç«¯æ•°å€¼
        pos_resized = pos_resized.clamp_(min=pos_chw_t.min().item(), max=pos_chw_t.max().item()) # åŒä¸‰æ¬¡æ’å€¼å¯èƒ½â€œè¿‡å†²â€ï¼ˆovershootï¼‰ï¼Œè¿™é‡ŒæŠŠæ•°å€¼è£å›åˆ°åŸ min/max èŒƒå›´ï¼Œä¿æŒåˆ†å¸ƒç¨³å®š

    # [1, C, H, W] -> [1, H, W, C] -> [H, W, C]
    pos_hwC = pos_resized.permute(0, 2, 3, 1).contiguous().numpy()
    return pos_hwC[0].astype(np.float32)


def _find_final_layernorm(_model):
    """åœ¨å¤šæ¡å…¼å®¹è·¯å¾„ä¸­æŸ¥æ‰¾æœ€ç»ˆ LayerNormï¼Œè¿”å›(åå­—, æ¨¡å—)"""
    candidates = [
        ("model.encoder.layernorm", getattr(_model.encoder, "layernorm", None)),
        ("model.encoder.layer_norm", getattr(_model.encoder, "layer_norm", None)),
        ("model.encoder.norm", getattr(_model.encoder, "norm", None)),
        ("model.layernorm", getattr(_model, "layernorm", None)),
        ("model.layer_norm", getattr(_model, "layer_norm", None)),
        ("model.norm", getattr(_model, "norm", None)),
        ("model.post_layernorm", getattr(_model, "post_layernorm", None)),
        ("model.post_layer_norm", getattr(_model, "post_layer_norm", None)),
    ]
    for name, mod in candidates:
        if mod is not None and hasattr(mod, "weight") and hasattr(mod, "bias"):
            return name, mod
    return None, None



# === æå–dinov2-baseé¢„è®­ç»ƒæƒé‡ ===
def export_dinov2_base(
    model_id_or_path: str,
    save_dir: str = "./weights_vit_base_224",
    image_size: int = 224,
    patch_size: int = 14,
):
    # === Step 0: åŸºç¡€é…ç½® & ç›®å½• ===
    print(" ")
    print("===== config =====")
    os.makedirs(save_dir, exist_ok=True)
    assert image_size % patch_size == 0, "image_size å¿…é¡»èƒ½è¢« patch_size æ•´é™¤"
    grid_h = grid_w = image_size // patch_size  # 224/14=16
    print(f"ã€configã€‘image_size={image_size}, patch_size={patch_size}, target_grid={grid_h}Ã—{grid_w}")

    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆç¡®ä¿æ— è®ºåœ¨å“ªè¿è¡Œéƒ½èƒ½æ‰¾åˆ°dinov2-baseï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ‹¼æ¥æ¨¡å‹è·¯å¾„ï¼šè„šæœ¬ç›®å½• + ä¼ å…¥çš„æ¨¡å‹å/è·¯å¾„
    model_path = os.path.join(script_dir, model_id_or_path)

    # === Step 1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé»˜è®¤ ./dinov2-baseï¼›ä¹Ÿå¯ä¼ å…¥ facebook/dinov2-baseï¼‰ ===
    print(" ")
    print("===== load Dinov2Model =====")
    model = Dinov2Model.from_pretrained(model_path, local_files_only=os.path.isdir(model_path))
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼hidden_size =", model.config.hidden_size, ", num_hidden_layers =", model.config.num_hidden_layers)

    # === Step 2: æå– Patch Embeddingï¼ˆConv2d 14Ã—14ï¼Œstride=14ï¼‰===
    print(" ")
    print("===== Patch Embedding =====")
    patch_proj = model.embeddings.patch_embeddings.projection  # nn.Conv2d
    patch_w = _to_numpy(patch_proj.weight)  # [768, 3, 14, 14]
    _save_np(os.path.join(save_dir, "patch_embed_W.npy"), patch_w, "âœ… patch_embed_W.npy ä¿å­˜ä¸º Conv2d æ ¼å¼ï¼Œ")
    # print("ã€debugã€‘patch_embed æƒé‡èŒƒæ•°:", float(np.linalg.norm(patch_w)))

    # ä¿å­˜bias
    if patch_proj.bias is not None:
        patch_b = _to_numpy(patch_proj.bias)  # [768]
        _save_np(os.path.join(save_dir, "patch_embed_b.npy"), patch_b, "âœ… patch_embed_b.npy ä¿å­˜ï¼Œ")

    # === Step 3: æå– Position Embeddingï¼ˆä» 37Ã—37 æ’å€¼åˆ° 16Ã—16ï¼‰+ CLS Token ===
    print(" ")
    print("===== position_embeddings =====")
    # 3.1 ä½ç½®ç¼–ç ï¼ˆå«CLSï¼‰
    # é€šå¸¸å½¢çŠ¶ä¸º [1, 1+N, 768]ï¼Œå…¶ä¸­ N=é¢„è®­ç»ƒpatchæ•°(å¦‚ 37Ã—37=1369)
    pos_full = _to_numpy(model.embeddings.position_embeddings)  # [1, 1+N, C]
    print("position_embeddings åŸå§‹ shape:", tuple(pos_full.shape))

    # 3.2 åˆ†ç¦» CLS / patch
    cls_pos = pos_full[:, :1, :]            # [1,1,C]
    patch_pos = pos_full[:, 1:, :]          # [1,N,C]
    print("patch_pos token æ•° =", patch_pos.shape[1], "ï¼ˆåº”ä¸ºå¹³æ–¹æ•°ï¼Œå¦‚37Ã—37=1369ï¼‰")

    # 3.3 æ’å€¼åˆ°ç›®æ ‡ç½‘æ ¼ï¼ˆ16Ã—16ï¼‰
    pos_2d = interpolate_pos_embed_to_grid(patch_pos, grid_h, grid_w)  # [16,16,768]
    _save_np(os.path.join(save_dir, "position_embed.npy"), pos_2d, "âœ… å·²å¯¼å‡º Position Embedding(äºŒç»´ç½‘æ ¼)")

    # 3.4 å•ç‹¬å¯¼å‡º CLS tokenï¼ˆæ³¨æ„ï¼šè¿™æ˜¯ CLS å‘é‡ï¼Œä¸æ˜¯ patch çš„ä½ç½®ç¼–ç  CLS ä½ï¼‰
    cls_token = _to_numpy(model.embeddings.cls_token)  # [1,1,768]
    _save_np(os.path.join(save_dir, "cls_token.npy"), cls_token, "âœ… å·²å¯¼å‡º CLS Tokenï¼Œ")

    # 3.5 å¯¼å‡º â€œCLS çš„ä½ç½®ç¼–ç â€ â€”â€” å¯¹é½å®˜æ–¹åšæ³•
    cls_pos_embed = cls_pos.astype(np.float32)  # [1,1,768]
    _save_np(os.path.join(save_dir, "position_embed_cls.npy"), cls_pos_embed, "âœ… å·²å¯¼å‡º CLS Pos Embeddingï¼Œ")

    # === Step 4: æå– Encoder æ¯å±‚å‚æ•°ï¼ˆå…¨éƒ¨è½¬ç½®ä¸ºâ€œå³ä¹˜â€æ–¹å‘ï¼‰===
    print(" ")
    print("===== Encoder =====")
    """
    è¯´æ˜ï¼šä½ çš„ NumPy å®ç°ä¸­ç»Ÿä¸€ä½¿ç”¨ X @ Wï¼Œæ‰€ä»¥è¿™é‡Œå°†æ‰€æœ‰ nn.Linear çš„æƒé‡ .t() å†ä¿å­˜ï¼š
    Q/K/V: [out,in] â†’ [in,out]ï¼Œå †å æˆ [3,768,768]
    out_proj(O): [out,in] â†’ [in,out] = [768,768]
    fc1: [3072,768] â†’ [768,3072]
    fc2: [768,3072] â†’ [3072,768]
    """
    # éå† ViT çš„ç¬¬ i ä¸ª Encoder Blockã€‚ç†è®ºä¸Šæ¯å±‚åŒ…å«ï¼šLN â†’ Self-Attention â†’ æ®‹å·® â†’ LN â†’ MLP â†’ æ®‹å·®ï¼ˆDINOv2 å¸¸è§ä¸º Pre-LNï¼Œå¹¶å¸¦æ®‹å·®ç¼©æ”¾ LayerScaleï¼‰ã€‚
    for i, layer in enumerate(model.encoder.layer):
        # --- Self-Attention Projections ---
        attn = layer.attention.attention  # å†…éƒ¨åŒ…å« query/key/valueï¼ˆnn.Linearï¼‰
        Wq = _to_numpy(attn.query.weight.t())
        Wk = _to_numpy(attn.key.weight.t())
        Wv = _to_numpy(attn.value.weight.t())
        qkv = np.stack([Wq, Wk, Wv], axis=0)  # [3,768,768]
        _save_np(os.path.join(save_dir, f"encoder_layer_{i}_qkv.npy"), qkv,
                 f"âœ… L{i:02d} QKV å·²ä¿å­˜ï¼Œ")

        # out projection
        out_dense = layer.attention.output.dense
        Wo = _to_numpy(out_dense.weight.t())  # [768,768] å³ä¹˜
        _save_np(os.path.join(save_dir, f"encoder_layer_{i}_output.npy"), Wo,
                 f"âœ… L{i:02d} Attention Output(æŠ•å½±) å·²ä¿å­˜ï¼Œ")

        # --- LayerNorm1 / LayerNorm2 ---
        ln1 = _get_attr(layer, ["layernorm_before", "ln1", "norm1", "pre_layernorm", "layer_norm1"])
        ln2 = _get_attr(layer, ["layernorm_after", "ln2", "norm2", "post_layernorm", "layer_norm2"])
        assert ln1 is not None and ln2 is not None, "æœªæ‰¾åˆ° LayerNorm1/2ï¼Œè¯·æ£€æŸ¥ transformers ç‰ˆæœ¬"
        ln1_np = np.stack([_to_numpy(ln1.weight), _to_numpy(ln1.bias)], axis=0)  # [2,768]
        ln2_np = np.stack([_to_numpy(ln2.weight), _to_numpy(ln2.bias)], axis=0)  # [2,768]
        _save_np(os.path.join(save_dir, f"encoder_layer_{i}_ln1.npy"), ln1_np,
                 f"âœ… L{i:02d} LayerNorm1 å·²ä¿å­˜ï¼Œ")
        _save_np(os.path.join(save_dir, f"encoder_layer_{i}_ln2.npy"), ln2_np,
                 f"âœ… L{i:02d} LayerNorm2 å·²ä¿å­˜ï¼Œ")

        # --- MLP: fc1 / fc2 ---
        fc1 = layer.mlp.fc1
        fc2 = layer.mlp.fc2
        W1 = _to_numpy(fc1.weight.t())  # [768,3072]
        W2 = _to_numpy(fc2.weight.t())  # [3072,768]
        _save_np(os.path.join(save_dir, f"encoder_layer_{i}_fc1.npy"), W1,
                 f"âœ… L{i:02d} MLP fc1 å·²ä¿å­˜ï¼Œ")
        _save_np(os.path.join(save_dir, f"encoder_layer_{i}_fc2.npy"), W2,
                 f"âœ… L{i:02d} MLP fc2 å·²ä¿å­˜ï¼Œ")

        # --- Q/K/V bias ---
        attn = layer.attention.attention  # æˆ–ä½ è§£æå‡ºæ¥çš„ attn_core
        q = _get_attr(attn, ["query"])
        k = _get_attr(attn, ["key"])
        v = _get_attr(attn, ["value"])
        qkv_linear = _get_attr(attn, ["qkv"])
        # æœ‰çš„å®ç° Q/K/V æ˜¯ä¸‰å±‚ Linearï¼Œå„è‡ªæœ‰ bias:[768]ï¼›æœ‰çš„å®ç°æ˜¯ èåˆçš„ qkv ä¸€å±‚ï¼Œbias:[3*768]ï¼Œéœ€è¦åˆ‡æˆä¸‰æ®µã€‚
        if q is not None and k is not None and v is not None:
            bqkv = np.stack([_to_numpy(q.bias), _to_numpy(k.bias), _to_numpy(v.bias)], axis=0)  # [3,768]
            _save_np(os.path.join(save_dir, f"encoder_layer_{i}_qkv_bias.npy"), bqkv, f"âœ… L{i:02d} QKV bias å·²ä¿å­˜ï¼Œ") # å®é™…æ¨¡å‹qkv bias
        elif qkv_linear is not None and getattr(qkv_linear, "bias", None) is not None:
            b_qkv = _to_numpy(qkv_linear.bias)  # [3*768]
            D = b_qkv.shape[0] // 3
            bqkv = np.stack([b_qkv[:D], b_qkv[D:2 * D], b_qkv[2 * D:]], axis=0)
            _save_np(os.path.join(save_dir, f"encoder_layer_{i}_qkv_bias.npy"), bqkv,
                     f"âœ… L{i:02d} QKV bias å·²ä¿å­˜ï¼ˆfusedï¼‰ï¼Œ")

        # --- out proj bias ---
        out_dense = _get_attr(layer.attention, ["output"])
        saved_out_bias = False
        if out_dense is not None and hasattr(out_dense, "dense") and getattr(out_dense.dense, "bias", None) is not None:
            _save_np(os.path.join(save_dir, f"encoder_layer_{i}_output_bias.npy"), _to_numpy(out_dense.dense.bias),
                     f"âœ… L{i:02d} out bias å·²ä¿å­˜ï¼Œ")
            saved_out_bias = True
        if not saved_out_bias:
            out_proj = _get_attr(attn, ["out_proj"])
            if out_proj is not None and getattr(out_proj, "bias", None) is not None:
                _save_np(os.path.join(save_dir, f"encoder_layer_{i}_output_bias.npy"), _to_numpy(out_proj.bias),
                         f"âœ… L{i:02d} out bias å·²ä¿å­˜ï¼Œ")

        # --- MLP bias ---
        if getattr(layer.mlp.fc1, "bias", None) is not None:
            _save_np(os.path.join(save_dir, f"encoder_layer_{i}_fc1_bias.npy"), _to_numpy(layer.mlp.fc1.bias),
                     f"âœ… L{i:02d} fc1 bias å·²ä¿å­˜ï¼Œ")
        if getattr(layer.mlp.fc2, "bias", None) is not None:
            _save_np(os.path.join(save_dir, f"encoder_layer_{i}_fc2_bias.npy"), _to_numpy(layer.mlp.fc2.bias),
                     f"âœ… L{i:02d} fc2 bias å·²ä¿å­˜ï¼Œ")

        # --- LayerScaleï¼ˆæ–°å¢ï¼‰ ---
        # å…¼å®¹å‘½åï¼šlayer_scale1/layer_scale2ï¼Œå‚æ•°åé€šå¸¸å« lambda1
        ls1_mod = _get_attr(layer, ["layer_scale1", "ls1", "gamma1"])
        ls2_mod = _get_attr(layer, ["layer_scale2", "ls2", "gamma2"])

        def _export_ls(mod, name):
            if mod is None:
                print(f"ã€warnã€‘æœªæ‰¾åˆ° {name}ï¼ˆæœ¬å±‚æ—  LayerScaleï¼Ÿï¼‰")
                return
            # å¸¸è§ç»“æ„ï¼šæœ‰ä¸€ä¸ª Parameter å« lambda1
            lam = getattr(mod, "lambda1", None)
            if lam is None:
                # æœ‰äº›å®ç°å¯èƒ½ç›´æ¥æ˜¯ weightï¼Œåšä¸ªå…œåº•
                lam = getattr(mod, "weight", None)
            if lam is None:
                print(f"ã€warnã€‘{name} æœªæ‰¾åˆ° lambda1/weight")
                return
            arr = _to_numpy(lam).reshape(-1).astype(np.float32)  # [768]
            _save_np(os.path.join(save_dir, f"encoder_layer_{i}_{name}.npy"), arr,
                     f"âœ… L{i:02d} {name} å·²ä¿å­˜ï¼Œ")

        _export_ls(ls1_mod, "ls1")
        _export_ls(ls2_mod, "ls2")

    # === Step 5: Encoder å°¾éƒ¨ LayerNormï¼ˆæ›´é²æ£’çš„å¤šè·¯å¾„æŸ¥æ‰¾ï¼‰ ===
    print(" ")
    print("===== Encoder å°¾éƒ¨ LayerNorm =====")
    ln_name, enc_norm_mod = _find_final_layernorm(model)
    assert enc_norm_mod is not None, "æœªæ‰¾åˆ° Encoder å°¾éƒ¨ LayerNormï¼ˆå·²å°è¯•å¤šæ¡å…¼å®¹è·¯å¾„ï¼‰"
    print(f"æœ€ç»ˆLayerNormæ¨¡å— = {ln_name}")

    enc_ln = np.stack([_to_numpy(enc_norm_mod.weight), _to_numpy(enc_norm_mod.bias)], axis=0)  # [2,768]
    _save_np(os.path.join(save_dir, "encoder_ln_final.npy"), enc_ln, "âœ… Encoder å°¾éƒ¨ LayerNorm å·²ä¿å­˜ï¼Œ")

    print("\nğŸ‰ å…¨éƒ¨æƒé‡å¯¼å‡ºå®Œæˆï¼ä¿å­˜ç›®å½•ï¼š", save_dir)


def main():
    """
    æŠŠè„šæœ¬åšæˆä¸€ä¸ªå¯é…ç½®çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆé»˜è®¤å€¼å³æ˜¯åˆç†é…ç½®ï¼‰ï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œè¦†ç›–å…¶ä¸­ä»»æ„å‚æ•°ï¼Œæœ€ç»ˆæŠŠå¯¹åº”æ¨¡å‹çš„æƒé‡æŒ‰ä½ å‰é¢å®šä¹‰çš„æ ¼å¼å¯¼å‡ºä¸º .npyã€‚
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-id", type=str, default="dinov2-base",
                        help="é¢„è®­ç»ƒæ¨¡å‹ç›®å½•æˆ–HuggingFaceçš„æ¨¡å‹åï¼Œå¦‚ ./dinov2-base æˆ– facebook/dinov2-base")
    parser.add_argument("--save-dir", type=str, default="./weights_vit_base_224",
                        help="å¯¼å‡ºçš„ .npy æƒé‡ä¿å­˜ç›®å½•")
    parser.add_argument("--image-size", type=int, default=224)
    parser.add_argument("--patch-size", type=int, default=14)
    args = parser.parse_args()

    export_dinov2_base(
        model_id_or_path=args.model_id,
        save_dir=args.save_dir,
        image_size=args.image_size,
        patch_size=args.patch_size,
    )


if __name__ == "__main__":
    # å°å‹ debugï¼šæ‰“å° torch / transformers ç‰ˆæœ¬ï¼Œç¡®è®¤ç¯å¢ƒ
    print("torch =", torch.__version__)
    try:
        import transformers
        print("transformers =", transformers.__version__)
    except Exception:
        pass
    main()
